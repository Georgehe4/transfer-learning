{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-12 10:03:42,043] Making new env: BreakoutDeterministic-v4\n",
      "/Users/george/anaconda3/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "from atari_helper import *\n",
    "from replay_memory import *\n",
    "from dqn import *\n",
    "\n",
    "# MODIFY ENVIRONMENT TO CHANGE YOUR OUTPUTS\n",
    "from environment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main DQN and target DQN networks:\n",
    "with tf.variable_scope('mainDQN'):\n",
    "    MAIN_DQN = DQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)   # (★★)\n",
    "with tf.variable_scope('targetDQN'):\n",
    "    TARGET_DQN = DQN(atari.env.action_space.n, HIDDEN)               # (★★)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()    \n",
    "\n",
    "MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
    "TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tensorboard\n",
    "LAYER_IDS = [\"conv1\", \"conv2\", \"conv3\", \"conv4\", \"denseAdvantage\", \n",
    "             \"denseAdvantageBias\", \"denseValue\", \"denseValueBias\"]\n",
    "\n",
    "# Scalar summaries for tensorboard: loss, average reward and evaluation score\n",
    "with tf.name_scope('Performance'):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name='loss_summary')\n",
    "    LOSS_SUMMARY = tf.summary.scalar('loss', LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name='reward_summary')\n",
    "    REWARD_SUMMARY = tf.summary.scalar('reward', REWARD_PH)\n",
    "    EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name='evaluation_summary')\n",
    "    EVAL_SCORE_SUMMARY = tf.summary.scalar('evaluation_score', EVAL_SCORE_PH)\n",
    "\n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "\n",
    "# Histogramm summaries for tensorboard: parameters\n",
    "with tf.name_scope('Parameters'):\n",
    "    ALL_PARAM_SUMMARIES = []\n",
    "    for i, Id in enumerate(LAYER_IDS):\n",
    "        with tf.name_scope('mainDQN/'):\n",
    "            MAIN_DQN_KERNEL = tf.summary.histogram(Id, tf.reshape(MAIN_DQN_VARS[i], shape=[-1]))\n",
    "        ALL_PARAM_SUMMARIES.extend([MAIN_DQN_KERNEL])\n",
    "PARAM_SUMMARIES = tf.summary.merge(ALL_PARAM_SUMMARIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(main_dqn,  main_dqn_vars, param_summaries, target_dqn=None, target_dqn_vars=[], trained_path = None, save_file = None, model_name=\"my_model\"):\n",
    "    # Trained path: The path (if provided) to look for the saved file from. EG: \"trained/pong/\"\n",
    "    # Save_file: The path (if provided) to save tf outputs to\n",
    "    # The model name\n",
    "    \"\"\"Contains the training and evaluation loops\"\"\"\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()  \n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BS)   # (★)\n",
    "    if (target_dqn != None):\n",
    "        network_updater = TargetNetworkUpdater(main_dqn_vars, target_dqn_vars)\n",
    "    action_getter = ActionGetter(atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        if trained_path != None:\n",
    "            saver = tf.train.import_meta_graph(trained_path+save_file)\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(trained_path))\n",
    "        else:\n",
    "            sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            \n",
    "            ########################\n",
    "            ####### Training #######\n",
    "            ########################\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    # (4★)\n",
    "                    action = action_getter.get_action(sess, frame_number, atari.state, main_dqn)   \n",
    "                    # (5★)\n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)  \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    # (7★) Store transition in the replay memory\n",
    "                    my_replay_memory.add_experience(action=action, \n",
    "                                                    frame=processed_new_frame[:, :, 0],\n",
    "                                                    reward=reward, \n",
    "                                                    terminal=terminal_life_lost)   \n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, main_dqn, target_dqn,\n",
    "                                     BS, gamma = DISCOUNT_FACTOR) # (8★)\n",
    "                        loss_list.append(loss)\n",
    "                    if target_dqn != None and frame_number % NETW_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        network_updater.update_networks(sess) # (9★)\n",
    "                    \n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                # Output the progress:\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Scalar summaries for tensorboard\n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, \n",
    "                                        feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                                   REWARD_PH:np.mean(rewards[-100:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                    # Histogramm summaries for tensorboard\n",
    "                    summ_param = sess.run(param_summaries)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "                    \n",
    "                    print(len(rewards), frame_number, np.mean(rewards[-100:]))\n",
    "                    with open('rewards.dat', 'a') as reward_file:\n",
    "                        print(len(rewards), frame_number, \n",
    "                              np.mean(rewards[-100:]), file=reward_file)\n",
    "            \n",
    "            ########################\n",
    "            ###### Evaluation ######\n",
    "            ########################\n",
    "            terminal = True\n",
    "            gif = True\n",
    "            frames_for_gif = []\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "               \n",
    "                # Fire (action 1), when a life was lost or the game just started, \n",
    "                # so that the agent does not stand around doing nothing. When playing \n",
    "                # with other environments, you might want to change this...\n",
    "                action = 1 if terminal_life_lost else action_getter.get_action(sess, frame_number,\n",
    "                                                                               atari.state, \n",
    "                                                                               main_dqn,\n",
    "                                                                               evaluation=True)\n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "\n",
    "                if gif: \n",
    "                    frames_for_gif.append(new_frame)\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                    gif = False # Save only the first game of the evaluation as a gif\n",
    "                     \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n",
    "            try:\n",
    "                generate_gif(frame_number, frames_for_gif, eval_rewards[0], PATH)\n",
    "            except IndexError:\n",
    "                print(\"No evaluation game finished\")\n",
    "            \n",
    "            #Save the network parameters\n",
    "            saver.save(sess, PATH+'/'+model_name, global_step=frame_number)\n",
    "            frames_for_gif = []\n",
    "            \n",
    "            # Show the evaluation score in tensorboard\n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            with open('rewardsEval.dat', 'a') as eval_reward_file:\n",
    "                print(frame_number, np.mean(eval_rewards), file=eval_reward_file)\n",
    "\n",
    "\n",
    "# Code from https://gist.github.com/iganichev/d2d8a0b1abc6b15d4a07de83171163d4\n",
    "# Load variables in if they are relevant\n",
    "# Currently transfers conv layers\n",
    "# vars_list is a set of tf.trainable_variables\n",
    "def optimistic_restore(session, save_file, vars_list):\n",
    "    reader = tf.train.NewCheckpointReader(save_file)\n",
    "    saved_shapes = reader.get_variable_to_shape_map()\n",
    "    var_names = sorted([(var.name, var.name.split(':')[0]) for\n",
    "                      var in vars_list\n",
    "                      if var.name.split(':')[0] in saved_shapes])\n",
    "    restore_vars = []\n",
    "    name2var = dict(zip(map(lambda x: x.name.split(':')[0],\n",
    "                          tf.global_variables()),\n",
    "                      tf.global_variables()))\n",
    "    with tf.variable_scope('', reuse=True):\n",
    "        for var_name, saved_var_name in var_names:\n",
    "            curr_var = name2var[saved_var_name]\n",
    "            var_shape = curr_var.get_shape().as_list()\n",
    "            if var_shape == saved_shapes[saved_var_name]:\n",
    "                restore_vars.append(curr_var)\n",
    "                \n",
    "    print(\"Restoring the following variables: {}\".format(restore_vars))\n",
    "    saver = tf.train.Saver(restore_vars)\n",
    "    saver.restore(session, save_file)\n",
    "\n",
    "# Train on GPU if a trained_path is provided\n",
    "def transfer_initialized_train_dqn(main_dqn,  main_dqn_vars, param_summaries, target_dqn=None, target_dqn_vars=[], trained_path = None, save_file = None, model_name=\"my_model\"):\n",
    "    # Trained path: The path (if provided) to look for the saved file from. EG: \"trained/pong/\"\n",
    "    # Save_file: The path (if provided) to save tf outputs to\n",
    "    # The model name\n",
    "    \"\"\"Contains the training and evaluation loops\"\"\"\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()  \n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BS)   # (★)\n",
    "    if (target_dqn != None):\n",
    "        network_updater = TargetNetworkUpdater(main_dqn_vars, target_dqn_vars)\n",
    "    action_getter = ActionGetter(atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        if trained_path != None:\n",
    "            # Init everything to start off unitialized weights\n",
    "            sess.run(init)\n",
    "            # Load saver for weights that can be transferred\n",
    "            # saver = tf.train.import_meta_graph(trained_path+save_file)\n",
    "            optimistic_restore(sess, tf.train.latest_checkpoint(trained_path), main_dqn_vars+target_dqn_vars)\n",
    "        \n",
    "        else:\n",
    "            sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            ########################\n",
    "            ####### Training #######\n",
    "            ########################\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    # (4★)\n",
    "                    action = action_getter.get_action(sess, frame_number, atari.state, main_dqn)   \n",
    "                    # (5★)\n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)  \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    # (7★) Store transition in the replay memory\n",
    "                    my_replay_memory.add_experience(action=action, \n",
    "                                                    frame=processed_new_frame[:, :, 0],\n",
    "                                                    reward=reward, \n",
    "                                                    terminal=terminal_life_lost)   \n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn_double_dqn(sess, my_replay_memory, main_dqn, target_dqn,\n",
    "                                     BS, gamma = DISCOUNT_FACTOR) # (8★)\n",
    "                        loss_list.append(loss)\n",
    "                    if target_dqn != None and frame_number % NETW_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        network_updater.update_networks(sess) # (9★)\n",
    "                    \n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                # Output the progress:\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Scalar summaries for tensorboard\n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(param_summaries, \n",
    "                                        feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                                   REWARD_PH:np.mean(rewards[-100:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                    # Histogramm summaries for tensorboard\n",
    "                    summ_param = sess.run(param_summaries)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "                    \n",
    "                    print(len(rewards), frame_number, np.mean(rewards[-100:]))\n",
    "                    with open('rewards.dat', 'a') as reward_file:\n",
    "                        print(len(rewards), frame_number, \n",
    "                              np.mean(rewards[-100:]), file=reward_file)\n",
    "            \n",
    "            ########################\n",
    "            ###### Evaluation ######\n",
    "            ########################\n",
    "            terminal = True\n",
    "            gif = True\n",
    "            frames_for_gif = []\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "               \n",
    "                # Fire (action 1), when a life was lost or the game just started, \n",
    "                # so that the agent does not stand around doing nothing. When playing \n",
    "                # with other environments, you might want to change this...\n",
    "                action = 1 if terminal_life_lost else action_getter.get_action(sess, frame_number,\n",
    "                                                                               atari.state, \n",
    "                                                                               main_dqn,\n",
    "                                                                               evaluation=True)\n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "\n",
    "                if gif: \n",
    "                    frames_for_gif.append(new_frame)\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                    gif = False # Save only the first game of the evaluation as a gif\n",
    "                     \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n",
    "            try:\n",
    "                generate_gif(frame_number, frames_for_gif, eval_rewards[0], PATH)\n",
    "            except IndexError:\n",
    "                print(\"No evaluation game finished\")\n",
    "            \n",
    "            #Save the network parameters\n",
    "            saver.save(sess, PATH+'/'+model_name, global_step=frame_number)\n",
    "            frames_for_gif = []\n",
    "            \n",
    "            # Show the evaluation score in tensorboard\n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            with open('rewardsEval.dat', 'a') as eval_reward_file:\n",
    "                print(frame_number, np.mean(eval_rewards), file=eval_reward_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and run trained network\n",
    "if TEST:\n",
    "    gif_path = \"GIF/\"\n",
    "    os.makedirs(gif_path,exist_ok=True)\n",
    "\n",
    "    if ENV_NAME == 'BreakoutDeterministic-v4':\n",
    "        trained_path = \"trained/breakout/\"\n",
    "        save_file = \"my_model-15845555.meta\"\n",
    "    \n",
    "    elif ENV_NAME == 'PongDeterministic-v4':\n",
    "        trained_path = \"trained/pong/\"\n",
    "        save_file = \"my_model-3217770.meta\"\n",
    "\n",
    "    action_getter = ActionGetter(atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(trained_path+save_file)\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(trained_path))\n",
    "        frames_for_gif = []\n",
    "        terminal_live_lost = atari.reset(sess, evaluation = True)\n",
    "        episode_reward_sum = 0\n",
    "        while True:\n",
    "            atari.env.render()\n",
    "            action = 1 if terminal_live_lost else action_getter.get_action(sess, 0, atari.state, \n",
    "                                                                           MAIN_DQN, \n",
    "                                                                           evaluation = True)\n",
    "            processed_new_frame, reward, terminal, terminal_live_lost, new_frame = atari.step(sess, action)\n",
    "            episode_reward_sum += reward\n",
    "            frames_for_gif.append(new_frame)\n",
    "            if terminal == True:\n",
    "                break\n",
    "        \n",
    "        atari.env.close()\n",
    "        print(\"The total reward is {}\".format(episode_reward_sum))\n",
    "        print(\"Creating gif...\")\n",
    "        generate_gif(0, frames_for_gif, episode_reward_sum, gif_path)\n",
    "        print(\"Gif created, check the folder {}\".format(gif_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring the following variables: [<tf.Variable 'mainDQN/conv1/kernel:0' shape=(8, 8, 4, 32) dtype=float32_ref>, <tf.Variable 'mainDQN/conv2/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>, <tf.Variable 'mainDQN/conv3/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'mainDQN/conv4/kernel:0' shape=(7, 7, 64, 1024) dtype=float32_ref>, <tf.Variable 'targetDQN/conv1/kernel:0' shape=(8, 8, 4, 32) dtype=float32_ref>, <tf.Variable 'targetDQN/conv2/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>, <tf.Variable 'targetDQN/conv3/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'targetDQN/conv4/kernel:0' shape=(7, 7, 64, 1024) dtype=float32_ref>]\n",
      "10 1615 0.7\n",
      "20 3409 0.95\n",
      "30 5166 1.0\n",
      "40 6921 1.0\n",
      "50 8763 1.06\n",
      "60 11040 1.2833333333333334\n",
      "70 12948 1.3\n",
      "80 14782 1.3\n",
      "90 16507 1.2555555555555555\n",
      "100 18424 1.28\n",
      "110 20205 1.32\n",
      "120 22394 1.42\n",
      "130 24120 1.39\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6fc5fd4714c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                \u001b[0mtrained_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                \u001b[0msave_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                model_name=\"transfer_learn_{}\".format(ENV_NAME))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-107cb5d9b56e>\u001b[0m in \u001b[0;36mtransfer_initialized_train_dqn\u001b[0;34m(main_dqn, main_dqn_vars, param_summaries, target_dqn, target_dqn_vars, trained_path, save_file, model_name)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_getter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0;31m# (5★)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                     \u001b[0mprocessed_new_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_life_lost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                     \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0mepoch_frame\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs234/transfer-learning/atari_helper.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, sess, action)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mprocessed_new_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_frame\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (6★)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_new_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (6★)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4526\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4527\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4528\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN_TRANSFER:\n",
    "    if ENV_NAME == 'PongDeterministic-v4':\n",
    "        # Swap paths\n",
    "        trained_path = \"trained/breakout/\"\n",
    "        save_file = \"my_model-15845555.meta\"\n",
    "    \n",
    "    elif ENV_NAME == 'BreakoutDeterministic-v4':\n",
    "        # Swap paths\n",
    "        trained_path = \"trained/pong/\"\n",
    "        save_file = \"my_model-3217770.meta\"\n",
    "    # Only reset conv layers\n",
    "    transfer_initialized_train_dqn(MAIN_DQN,[v for v in MAIN_DQN_VARS if \"conv\" in v.name], PARAM_SUMMARIES, \n",
    "                               target_dqn = TARGET_DQN, \n",
    "                               target_dqn_vars = [v for v in TARGET_DQN_VARS if \"conv\" in v.name],\n",
    "                               trained_path = trained_path, \n",
    "                               save_file = save_file, \n",
    "                               model_name=\"transfer_learn_{}\".format(ENV_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused for now\n",
    "optimizer = MAIN_DQN.optimizer\n",
    "reset_main_optimizer_op = tf.variables_initializer(optimizer.variables())\n",
    "\n",
    "optimizer = TARGET_DQN.optimizer\n",
    "reset_target_optimizer_op = tf.variables_initializer(optimizer.variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jupyter-nbconvert --to script DQN.ipynb` to generate python\n",
    "\n",
    "`tensorboard --logdir=summaries` to set up tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning environment is provided by OpenAi's `gym`. It is very important that you have the right version of the environments. `BreakoutDeterministic-v3` for example has six actions whereas `BreakoutDeterministic-v4` has a minimal set of four actions, which is what DeepMind used in [xitari](https://github.com/deepmind/xitari/blob/master/games/supported/Breakout.cpp#L88-L91). Additional actions make the learning task harder for the agent which can alter the evaluation score significantly. If you want to find out the number of actions and their meaning, type `env.action_space.n` and  `env.unwrapped.get_action_meanings()`.\n",
    "\n",
    "There are two additional small adjustments we need to discuss:\n",
    "\n",
    "When a life is lost, we save `terminal_life_lost = True` in the replay memory. Create a new notebook, make a Breakout environment, in a loop repeat random or no actions and print the reward and the number of lives the agent has. \n",
    "\n",
    "`\n",
    "frame = env.reset()\n",
    "for i in range(1000):\n",
    "    new_frame, reward, terminal, info = env.step(0)\n",
    "    print(reward, terminal, info['ale.lives'])\n",
    "`\n",
    "\n",
    "You will see, that there is no punishment (reward is 0) when a life is lost. It helps the agent tremendously avoiding losing a life if you consider loss of life as end of episode. However, we only do this in the replay memory as we do not want to reset the game once the first life is lost. Therefore two terminal states `terminal` and `terminal_life_lost` are needed, one to reset the game, the other for the replay memory. This adjustment helped the agent improve from an average reward slightly above 50 to approximately 140 in Breakout! \n",
    "\n",
    "Let's wrap the `gym` environment in an `Atari` class which takes care of stacking frames ontop of each other to create states, resetting the environment when an episode ended and checking if a life was lost after a step was taken. You find the implementation in the cell below.\n",
    "\n",
    "During evaluation, at the beginning of each episode, action 1 ('FIRE') is repeated for a random number of steps between 1 and `no_op_steps=10`. This ensures, that the agent starts in a different situation every time and thus cannot simply learn a fixed sequence of actions. [Mnih et al. 2015](https://www.nature.com/articles/nature14236/) use a random number between 1 and 30 of 'NOOP'-actions (see page 10, Table 1). However, in Breakout, nothing happens if you don't fire first. Once there is a ball in the game, 'FIRE' does nothing. Therefore I started with a random number of 'FIRE'-actions. Furthermore, I limited the random number of initial 'FIRE' actions to 10. When experimenting with larger numbers, I found, that the first life was usually already lost when the agent starting moving. You might want to change this, in case you want to experiment with another environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know how the DQN predicts the best action and we have a simple answer to the exploration exploitation dilemma.\n",
    "So, what else do we need to make it work? Let's take a look at the algorithm presented on page 7 in [Mnih et al. 2015](https://www.nature.com/articles/nature14236/).\n",
    "\n",
    "![](pictures/DQN.png)\n",
    "\n",
    "Let us go through the algorithm step by step:\n",
    "* We do not know yet what *replay memory* D is.\n",
    "* Action-value function Q is our DQN network, that we already implemented.\n",
    "* We need to discuss why a second Q network called *target* action-value function is needed.\n",
    "* At the beginning of each episode a sequence is initalized. This is implemented by stacking four (grayscale) frames together as discussed above.\n",
    "* We discussed how the action is selected ($\\epsilon$-greedy).\n",
    "* When the action is performed, the environment returns the next frame and the reward for that action. `gym` additionaly returns a boolean called `terminal` that states whether the game is over and a dictionary containing the number of lives the agent has left (`ale.lives`). \n",
    "* We do not know yet, what it means to store a transition in the replay memory D. A list `[state, action, reward, terminal, new_state]` is called transition. A `state` are four frames stacked together. `new_state` is produced by stacking the observed frame (after the action is performed) onto `state` and removing the oldest frame. You will see the implementation later. \n",
    "* We have to discuss, how a minibatch is retured from the replay memory and how the gradient descend step is performed.\n",
    "* Finally we have to look at why and how the target Q network is reset to the main Q network.\n",
    "\n",
    "Let's continue with the replay memory\n",
    "\n",
    "## 5. Replay memory\n",
    "\n",
    ">Second, learning directly from consecutive samples is inefficient, due to the strong correlations between the samples; randomizing the samples breaks these correlations and therefore reduces the variance of the updates. Third, when learning on-policy the current parameters determine the next data sample that the parameters are trained on. For example, if the maximizing action is to move left then the training samples will be dominated by samples from the left-hand side; if the maximizing action then switches to the right then the training distribution will also switch. It is easy to see how unwanted feedback loops may arise and the parameters could get stuck in a poor local minimum, or even diverge catastrophically. ([page 5 of Mnih et al. 2013](https://arxiv.org/abs/1312.5602))\n",
    "\n",
    "This means that when we choose an action and perform a step to receive a reward, the network does not learn from this last step but rather adds the transition to the replay memory. It then draws a random minibatch from the replay memory to perform a gradient descent step.\n",
    "\n",
    "The replay memory stores the last one million transitions. Let's recall that a transition is `[state, action, reward, terminal, new_state]`. We therefore need to store the last one million `state`, `action`, `reward`, `terminal` and `new_state`. If you remember that `state` and `new_state` are four frames each, that would be eight million frames. However, since `new_state` is created by stacking the newest frame on top of `state` and deleting the oldest frame, `new_state` and `state` share three frames. Furthermore, `new_state` of transition i will be `state` of transition i+1. This means that it is sufficient to store the last one million frames (84*84 pixels) as a (1 million, 84, 84) tensor and then slicing four frames out of this tensor when we need a `state` or `new_state`. \n",
    "\n",
    "With one million frames of 84 by 84 pixels that need to be stored in your computers memory, we need to consider in what datatype we store them. The environment returns frames with pixel values stored as `uint8` which can have values ranging from 0 to 255. A `uint8` needs 8 bits. The network expects a `tf.float32` input with pixel values between 0 and 1 (which takes four times more space than a `uint8`). Since we want to reduce the memory requirements, we store the frames in `uint8` and divide them by 255 before passing them to the network.\n",
    "\n",
    "When implementing this version of replay memory, we looked at this [code](https://github.com/tambetm/simple_dqn/blob/master/src/replay_memory.py) and ended up implementing the replay memory with some adjustments that make the code more understandable.\n",
    "\n",
    "Let's look at the `ReplayMemory` class below. In the constructor, we pre-allocate the memory for the frames, the actions, the rewards, the terminal states and also for the states and new states of the minibatch. \n",
    "\n",
    "In the `add_experience` method the frames etc. are written into `self.frames` at index `self.current` which is then increased by 1. When `self.current` reaches the size of the replay memory (one million), it is reset to zero to overwrite the oldest frames. The method `_get_state` slices four frames out of `self.frames` and returns them as a `state`.\n",
    "\n",
    "To understand what the method `_get_valid_indices` does, we need to understand what an invalid index is. We store all frames the agent sees in `self.frames`. When a game terminates (`terminal=True`) at index i, frame at index i belongs to a different episode than the frame at i+1. We want to avoid creating a `state` with frames from two different episodes. The same thing can happen at the index `self.current`. \n",
    "\n",
    "Finally we need to make sure that an index is not smaller than the number of frames stacked toghether to create a `state` (`self.agent_history_length=4`), so that a `state` and `new_state` can be sliced out of the array. \n",
    "\n",
    "The method `_get_valid_indices` finds 32 (size of minibatch) valid indices.\n",
    "The method `get_minibatch` returns the transitions for those indices. Pay attention that we need to transpose `self.states` and `self.new_states` before returning them: the DQN expects an input of the dimension `[None,84,84,4]` whereas `_get_state` returns a `state` of the dimension `[4,84,84]`\n",
    "\n",
    "We now know 1) why a replay memory greatly improves the stability of the algorithm, 2) how to store a transition in the replay memory and 3) how a minibatch is returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target network and parameter update\n",
    "\n",
    "Why do we need two networks, the action-value function and the *target* action-value function?\n",
    "\n",
    "Remember that prior to updating the network's parameters, we draw a minibatch with 32 transitions. For simplicity we consider one transition now. It consists of a `state`, an `action` that was performed in the `state`, the received `reward`, the `new_state` and a bool saying whether the episode is over.\n",
    "\n",
    "We perform a gradient descent step:\n",
    "The main network looks at state and estimates the $Q_\\text{prediction}$-values that say how good each action is. However, we want the $Q$-values to follow the Bellman equation we introduced above. Therefore we calculate the $Q_\\text{target}$-values according to the Bellman equation (how we would like the $Q$-values to be) and then compare the estimates $Q_\\text{prediction}$ to the targets $Q_\\text{target}$. Let's consider the quadratic loss function instead of the Huber loss function for simplicity:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{2}\\left(Q_\\text{prediction} - Q_\\text{target}\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "This ensures that we regress the current $Q_\\text{prediction}$-values for `state` towards the $Q_\\text{target}$-values given by the Bellman equation.\n",
    "\n",
    "$Q_\\text{prediction}$ is calculated in the `DQN` class (`self.q_values`). $Q_\\text{prediction}$ depends on the current `state` in the minibatch we drew and on the parameters $\\theta$ of the network that estimates it.\n",
    "\n",
    "The $Q_\\text{target}$ value is calculated according to the Bellman equation. It is the sum of the immediate reward $r$ received for performing action $a$ in state $s$ (`action` and `state` from the minibatch) and the maximum $Q$-value over all possible actions $a'$ in $s'$ (`new_state` from the minibatch):\n",
    "\n",
    "\\begin{equation}\n",
    "Q_\\text{target}(s,a) = r + \\gamma \\textrm{max} \\left(Q(s',a')\\right)\n",
    "\\end{equation}\n",
    "\n",
    "This is not done in the `DQN` class but in the `learn` method below. The calculated value is then passed to the placeholder called `self.target_q` in the `DQN` class. There, the loss function is defined and the gradient descent step is performed.\n",
    "\n",
    "So, now that we understand how the parameters are updated, why use two networks?\n",
    "\n",
    "The problem is that both $Q_\\text{prediction}$ and $Q_\\text{target}$ depend on the same parameters $\\theta$ if only one network is used. This can lead to instability when regressing $Q_\\text{prediction}$ towards $Q_\\text{target}$ because the \"target is moving\". We ensure a \"fixed target\" by introducing a second network with fixed and only occasionally updated parameters that estimates the target $Q$-values.\n",
    "\n",
    "On page 1 of [Mnih et al. 2015](https://www.nature.com/articles/nature14236/) the authors explain:\n",
    ">Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (also known as Q) function. This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations between the action-values [...] and the target values [...].\n",
    "We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution [...]. Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target. \n",
    "\n",
    "Therefore they used one network to predict the $Q_\\text{prediction}$-value and the other fixed network to predict the $Q_\\text{target}$-value. The main network is optimized during the gradient descend step and every 10000 steps the main network's parameters are copied to the target network. Be aware that the network update frequency is measured in the number of chosen actions/frames seen (DeepMind code) and not the number of parameter updates which occur every four frames ([Mnih et al. 2015](https://www.nature.com/articles/nature14236/)).\n",
    "\n",
    "There is one additional very powerful improvement called *Double Q-Learning*.\n",
    "\n",
    "## 7. Double Q-Learning\n",
    "DQN has been observed to estimate unrealistically high $Q$-values. The reason for this is, that the Bellman equation *includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values* (see [van Hasselt et al. 2016, page 1](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)). \n",
    "\n",
    "The authors explain:\n",
    "\n",
    ">If all values would be uniformly higher then the relative action preferences are preserved and we would not expect the resulting policy to be any worse. [...]\n",
    "If, however, the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might negatively affect the quality of the resulting policy. [...]\n",
    "We then show that this algorithm not only yields more accurate value estimates, but leads to much higher scores on several games. This demonstrates that the overestimations of DQN were indeed leading to poorer policies and that it is beneficial to reduce them \n",
    "\n",
    "The estimated $Q$-values are noisy. Assume that the true $Q$-value is 0 for all actions. But because of the noisy estimation, some $Q$-values might be slightly positive, others slightly negative. The max operation in the Bellman equation will however always chose the small positive values, despite the fact, that those actions are not truly better. The estimatation of $Q$-values is thus biased towards larger values. How do we fix this?\n",
    "Instead of estimating the $Q$-values in the next state $Q(s',a')$ with only the target network, we use the main network to estimate which action is the best and then ask the target network how high the $Q$-value is for that action. This way, the main network will still prefer the action with the small positive $Q$-value but because of the noisy estimation, the target network will predict a small positive **or** small negative $Q$-value for that action and on average, the predicted $Q$-values will be closer to 0.\n",
    "\n",
    "Mathematically, the reason for the overestimation is, that the expectation of a maximum is greater than or equal to the maximum of an expectation [van Hasselt 2013, Theorem 1](https://arxiv.org/abs/1302.7175).\n",
    "\n",
    "The Bellman equation changes from\n",
    "\n",
    "\\begin{align}\n",
    "Q_\\text{target}(s,a) &= r + \\gamma \\textrm{max} Q(s',a';\\theta_\\text{target}) &\\text{Normal DQN}\\\\\n",
    "\\text{to}\\qquad\\qquad Q_\\text{target}(s,a) &= r + \\gamma Q\\left(s',a'=\\text{argmax} Q(s',a';\\theta_\\text{main});\\theta_\\text{target}\\right)&\\text{Double DQN}\n",
    "\\end{align}\n",
    "\n",
    "The main network estimates which action $a'$ (in the next state $s'$) is best (that is the $\\text{argmax} Q(s',a';\\theta_\\text{main})$ part). The target network then estimates what the $Q$-value for that action is. This $Q$-value has to be discounted with $\\gamma$ and is then added to the reward $r$ the agent got for action $a$ (not $a'$).\n",
    "\n",
    "I know that this equation might look discouraging. So let's describe it again in words:\n",
    "\n",
    "Normal DQN: Ask the target network for the highest $Q$-Value. If the noisy $Q$-values are for example $(0.1,-0.1)$ for actions with index $0$ and $1$ respectively, the target $Q$-network will answer $0.1$.\n",
    "\n",
    "Double DQN: Ask the main network which action has the highest $Q$-value. If the noisy $Q$-values are for example $(0.1,-0.1)$ for actions with index $0$ and $1$ respectively, the main network will answer that action with index $0$ has the highest $Q$-value. Then we ask the target network, which has a different noise, what the $Q$-value for the action with the chosen index ($0$ in this example) is. Let's assume the target network's noisy estimates are $(-0.05,0.3)$ it will answer $-0.05$.  \n",
    "\n",
    "This solves the problem of overestimated $Q$-values because the two networks have different noise and the bias towards slightly larger noisy $Q$-values cancels.\n",
    "\n",
    "\n",
    "One more thing:\n",
    "If the game is over (`terminal=True`) because the agend lost or won, there is no next state and the $Q_\\text{target}$-value is simply the reward $r$.\n",
    "\n",
    "Look at the implementation in the cell below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
