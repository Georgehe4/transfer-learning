{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "TEST = False\n",
    "TRAIN_TRANSFER = True\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "#ENV_NAME = 'PongDeterministic-v4'  \n",
    "# You can increase the learning rate to 0.00025 in Pong for quicker results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of DeepMind's Deep Q-Learning by Fabio M. Graetz, 2018\n",
    "If you have questions or suggestions, write me a mail fabiograetzatgooglemaildotcom\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFrame:\n",
    "    \"\"\"Resizes and converts RGB Atari frames to grayscale\"\"\"\n",
    "    def __init__(self, frame_height=84, frame_width=84):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "        \"\"\"\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, \n",
    "                                                [self.frame_height, self.frame_width], \n",
    "                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def process(self, session, frame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            session: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\"Implements a Deep Q Network\"\"\"\n",
    "    \n",
    "    # pylint: disable=too-many-instance-attributes\n",
    "    \n",
    "    def __init__(self, n_actions, hidden=1024, learning_rate=0.00001, \n",
    "                 frame_height=84, frame_width=84, agent_history_length=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_actions: Integer, number of possible actions\n",
    "            hidden: Integer, Number of filters in the final convolutional layer. \n",
    "                    This is different from the DeepMind implementation\n",
    "            learning_rate: Float, Learning rate for the Adam optimizer\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.frame_height, \n",
    "                                           self.frame_width, self.agent_history_length], \n",
    "                                    dtype=tf.float32)\n",
    "        # Normalizing the input\n",
    "        self.inputscaled = self.input/255\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs=self.inputscaled, filters=32, kernel_size=[8, 8], strides=4,\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n",
    "        self.conv4 = tf.layers.conv2d(\n",
    "            inputs=self.conv3, filters=hidden, kernel_size=[7, 7], strides=1, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n",
    "        \n",
    "        # Splitting into value and advantage stream\n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.advantage = tf.layers.dense(\n",
    "            inputs=self.advantagestream, units=self.n_actions,\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage\")\n",
    "        self.value = tf.layers.dense(\n",
    "            inputs=self.valuestream, units=1, \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name='value')\n",
    "        \n",
    "        # Combining value and advantage into Q-values as described above\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        # The next lines perform the parameter update. This will be explained in detail later.\n",
    "        \n",
    "        # targetQ according to Bellman equation: \n",
    "        # Q = r + gamma*max Q', calculated in the function learn()\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        # Action that was performed\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        # Q value of the action that was performed\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.n_actions, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        # Parameter updates\n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGetter:\n",
    "    \"\"\"Determines an action according to an epsilon greedy strategy with annealing epsilon\"\"\"\n",
    "    def __init__(self, n_actions, eps_initial=1, eps_final=0.1, eps_final_frame=0.01, \n",
    "                 eps_evaluation=0.0, eps_annealing_frames=1000000, \n",
    "                 replay_memory_start_size=50000, max_frames=25000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_actions: Integer, number of possible actions\n",
    "            eps_initial: Float, Exploration probability for the first \n",
    "                replay_memory_start_size frames\n",
    "            eps_final: Float, Exploration probability after \n",
    "                replay_memory_start_size + eps_annealing_frames frames\n",
    "            eps_final_frame: Float, Exploration probability after max_frames frames\n",
    "            eps_evaluation: Float, Exploration probability during evaluation\n",
    "            eps_annealing_frames: Int, Number of frames over which the \n",
    "                exploration probabilty is annealed from eps_initial to eps_final\n",
    "            replay_memory_start_size: Integer, Number of frames during \n",
    "                which the agent only explores\n",
    "            max_frames: Integer, Total number of frames shown to the agent\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "        self.replay_memory_start_size = replay_memory_start_size\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        self.slope = -(self.eps_initial - self.eps_final)/self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_memory_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame)/(self.max_frames - self.eps_annealing_frames - self.replay_memory_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "\n",
    "    def get_action(self, session, frame_number, state, main_dqn, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            session: A tensorflow session object\n",
    "            frame_number: Integer, number of the current frame\n",
    "            state: A (84, 84, 4) sequence of frames of an Atari game in grayscale\n",
    "            main_dqn: A DQN object\n",
    "            evaluation: A boolean saying whether the agent is being evaluated\n",
    "        Returns:\n",
    "            An integer between 0 and n_actions - 1 determining the action the agent perfoms next\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            eps = self.eps_evaluation\n",
    "        elif frame_number < self.replay_memory_start_size:\n",
    "            eps = self.eps_initial\n",
    "        elif frame_number >= self.replay_memory_start_size and frame_number < self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope_2*frame_number + self.intercept_2\n",
    "        \n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        return session.run(main_dqn.best_action, feed_dict={main_dqn.input:[state]})[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"Replay Memory that stores the last size=1,000,000 transitions\"\"\"\n",
    "    def __init__(self, size=1000000, frame_height=84, frame_width=84, \n",
    "                 agent_history_length=4, batch_size=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: Integer, Number of stored transitions\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
    "            batch_size: Integer, Number if transitions returned in a minibatch\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the states and new_states in a minibatch\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: An integer between 0 and env.action_space.n - 1 \n",
    "                determining the action the agent perfomed\n",
    "            frame: A (84, 84, 1) frame of an Atari game in grayscale\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frame_height, self.frame_width):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "             \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
    "        \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "            \n",
    "    def get_minibatch(self):\n",
    "        \"\"\"\n",
    "        Returns a minibatch of self.batch_size = 32 transitions\n",
    "        \"\"\"\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "        \n",
    "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_dqn, target_dqn, batch_size, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: A tensorflow sesson object\n",
    "        replay_memory: A ReplayMemory object\n",
    "        main_dqn: A DQN object\n",
    "        target_dqn: A DQN object\n",
    "        batch_size: Integer, Batch size\n",
    "        gamma: Float, discount factor for the Bellman equation\n",
    "    Returns:\n",
    "        loss: The loss of the minibatch, for tensorboard\n",
    "    Draws a minibatch from the replay memory, calculates the \n",
    "    target Q-value that the prediction Q-value is regressed to. \n",
    "    Then a parameter update is performed on the main DQN.\n",
    "    \"\"\"\n",
    "    # Draw a minibatch from the replay memory\n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()    \n",
    "    # The main network estimates which action is best (in the next \n",
    "    # state s', new_states is passed!) \n",
    "    # for every transition in the minibatch\n",
    "    arg_q_max = session.run(main_dqn.best_action, feed_dict={main_dqn.input:new_states})\n",
    "    # The target network estimates the Q-values (in the next state s', new_states is passed!) \n",
    "    # for every transition in the minibatch\n",
    "    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), arg_q_max]\n",
    "    # Bellman equation. Multiplication with (1-terminal_flags) makes sure that \n",
    "    # if the game is over, targetQ=rewards\n",
    "    target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
    "    # Gradient descend step to update the parameters of the main network\n",
    "    loss, _ = session.run([main_dqn.loss, main_dqn.update], \n",
    "                          feed_dict={main_dqn.input:states, \n",
    "                                     main_dqn.target_q:target_q, \n",
    "                                     main_dqn.action:actions})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater:\n",
    "    \"\"\"Copies the parameters of the main DQN to the target DQN\"\"\"\n",
    "    def __init__(self, main_dqn_vars, target_dqn_vars):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            main_dqn_vars: A list of tensorflow variables belonging to the main DQN network\n",
    "            target_dqn_vars: A list of tensorflow variables belonging to the target DQN network\n",
    "        \"\"\"\n",
    "        self.main_dqn_vars = main_dqn_vars\n",
    "        self.target_dqn_vars = target_dqn_vars\n",
    "\n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_dqn_vars):\n",
    "            copy_op = self.target_dqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "            \n",
    "    def update_networks(self, sess):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "        Assigns the values of the parameters of the main network to the \n",
    "        parameters of the target network\n",
    "        \"\"\"\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(frame_number, frames_for_gif, reward, path):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            frame_number: Integer, determining the number of the current frame\n",
    "            frames_for_gif: A sequence of (210, 160, 3) frames of an Atari game in RGB\n",
    "            reward: Integer, Total reward of the episode that es ouputted as a gif\n",
    "            path: String, path where gif is saved\n",
    "    \"\"\"\n",
    "    for idx, frame_idx in enumerate(frames_for_gif): \n",
    "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
    "                                     preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{path}{\"ATARI_frame_{0}_reward_{1}.gif\".format(frame_number, reward)}', \n",
    "                    frames_for_gif, duration=1/30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari:\n",
    "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
    "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
    "        self.env = gym.make(envName)\n",
    "        self.frame_processor = ProcessFrame()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.agent_history_length = agent_history_length\n",
    "\n",
    "    def reset(self, sess, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            evaluation: A boolean saying whether the agent is evaluating or training\n",
    "        Resets the environment and stacks four frames ontop of each other to \n",
    "        create the first state\n",
    "        \"\"\"\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True # Set to true so that the agent starts \n",
    "                                  # with a 'FIRE' action when evaluating\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
    "        processed_frame = self.frame_processor.process(sess, frame)   # (★★★)\n",
    "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "\n",
    "    def step(self, sess, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            action: Integer, action the agent performs\n",
    "        Performs an action and observes the reward and terminal state from the environment\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)  # (5★)\n",
    "            \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.frame_processor.process(sess, new_frame)   # (6★)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-12 16:11:45,002] Making new env: BreakoutDeterministic-v4\n",
      "/Users/george/anaconda3/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Control parameters\n",
    "MAX_EPISODE_LENGTH = 18000       # Equivalent of 5 minutes of gameplay at 60 frames per second\n",
    "EVAL_FREQUENCY = 200000          # Number of frames the agent sees between evaluations\n",
    "EVAL_STEPS = 10000               # Number of frames for one evaluation\n",
    "NETW_UPDATE_FREQ = 10000         # Number of chosen actions between updating the target network. \n",
    "                                 # According to Mnih et al. 2015 this is measured in the number of \n",
    "                                 # parameter updates (every four actions), however, in the \n",
    "                                 # DeepMind code, it is clearly measured in the number\n",
    "                                 # of actions the agent choses\n",
    "DISCOUNT_FACTOR = 0.99           # gamma in the Bellman equation\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # Number of completely random actions, \n",
    "                                 # before the agent starts learning\n",
    "MAX_FRAMES = 30000000            # Total number of frames the agent sees \n",
    "MEMORY_SIZE = 1000000            # Number of transitions stored in the replay memory\n",
    "NO_OP_STEPS = 10                 # Number of 'NOOP' or 'FIRE' actions at the beginning of an \n",
    "                                 # evaluation episode\n",
    "UPDATE_FREQ = 4                  # Every four actions a gradient descend step is performed\n",
    "HIDDEN = 1024                    # Number of filters in the final convolutional layer. The output \n",
    "                                 # has the shape (1,1,1024) which is split into two streams. Both \n",
    "                                 # the advantage stream and value stream have the shape \n",
    "                                 # (1,1,512). This is slightly different from the original \n",
    "                                 # implementation but tests I did with the environment Pong \n",
    "                                 # have shown that this way the score increases more quickly\n",
    "LEARNING_RATE = 0.00001          # Set to 0.00025 in Pong for quicker results. \n",
    "                                 # Hessel et al. 2017 used 0.0000625\n",
    "BS = 32                          # Batch size\n",
    "\n",
    "PATH = \"output/\"                 # Gifs and checkpoints will be saved here\n",
    "SUMMARIES = \"summaries\"          # logdir for tensorboard\n",
    "RUNID = 'run_1'\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(SUMMARIES, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(SUMMARIES, RUNID))\n",
    "\n",
    "atari = Atari(ENV_NAME, NO_OP_STEPS)\n",
    "\n",
    "print(\"The environment has the following {} actions: {}\".format(atari.env.action_space.n, \n",
    "                                                                atari.env.unwrapped.get_action_meanings()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main DQN and target DQN networks:\n",
    "with tf.variable_scope('mainDQN'):\n",
    "    MAIN_DQN = DQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)   # (★★)\n",
    "with tf.variable_scope('targetDQN'):\n",
    "    TARGET_DQN = DQN(atari.env.action_space.n, HIDDEN)               # (★★)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()    \n",
    "\n",
    "MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
    "TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDS = [\"conv1\", \"conv2\", \"conv3\", \"conv4\", \"denseAdvantage\", \n",
    "             \"denseAdvantageBias\", \"denseValue\", \"denseValueBias\"]\n",
    "\n",
    "# Scalar summaries for tensorboard: loss, average reward and evaluation score\n",
    "with tf.name_scope('Performance'):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name='loss_summary')\n",
    "    LOSS_SUMMARY = tf.summary.scalar('loss', LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name='reward_summary')\n",
    "    REWARD_SUMMARY = tf.summary.scalar('reward', REWARD_PH)\n",
    "    EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name='evaluation_summary')\n",
    "    EVAL_SCORE_SUMMARY = tf.summary.scalar('evaluation_score', EVAL_SCORE_PH)\n",
    "\n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "\n",
    "# Histogramm summaries for tensorboard: parameters\n",
    "with tf.name_scope('Parameters'):\n",
    "    ALL_PARAM_SUMMARIES = []\n",
    "    for i, Id in enumerate(LAYER_IDS):\n",
    "        with tf.name_scope('mainDQN/'):\n",
    "            MAIN_DQN_KERNEL = tf.summary.histogram(Id, tf.reshape(MAIN_DQN_VARS[i], shape=[-1]))\n",
    "        ALL_PARAM_SUMMARIES.extend([MAIN_DQN_KERNEL])\n",
    "PARAM_SUMMARIES = tf.summary.merge(ALL_PARAM_SUMMARIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Contains the training and evaluation loops\"\"\"\n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BS)   # (★)\n",
    "    network_updater = TargetNetworkUpdater(MAIN_DQN_VARS, TARGET_DQN_VARS)\n",
    "    action_getter = ActionGetter(atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            \n",
    "            ########################\n",
    "            ####### Training #######\n",
    "            ########################\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    # (4★)\n",
    "                    action = action_getter.get_action(sess, frame_number, atari.state, MAIN_DQN)   \n",
    "                    # (5★)\n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)  \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    # (7★) Store transition in the replay memory\n",
    "                    my_replay_memory.add_experience(action=action, \n",
    "                                                    frame=processed_new_frame[:, :, 0],\n",
    "                                                    reward=reward, \n",
    "                                                    terminal=terminal_life_lost)   \n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DQN, TARGET_DQN,\n",
    "                                     BS, gamma = DISCOUNT_FACTOR) # (8★)\n",
    "                        loss_list.append(loss)\n",
    "                    if frame_number % NETW_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        network_updater.update_networks(sess) # (9★)\n",
    "                    \n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                # Output the progress:\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Scalar summaries for tensorboard\n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                                   REWARD_PH:np.mean(rewards[-100:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                    # Histogramm summaries for tensorboard\n",
    "                    summ_param = sess.run(PARAM_SUMMARIES)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "                    \n",
    "                    print(len(rewards), frame_number, np.mean(rewards[-100:]))\n",
    "                    with open('rewards.dat', 'a') as reward_file:\n",
    "                        print(len(rewards), frame_number, \n",
    "                              np.mean(rewards[-100:]), file=reward_file)\n",
    "            \n",
    "            ########################\n",
    "            ###### Evaluation ######\n",
    "            ########################\n",
    "            terminal = True\n",
    "            gif = True\n",
    "            frames_for_gif = []\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "               \n",
    "                # Fire (action 1), when a life was lost or the game just started, \n",
    "                # so that the agent does not stand around doing nothing. When playing \n",
    "                # with other environments, you might want to change this...\n",
    "                action = 1 if terminal_life_lost else action_getter.get_action(sess, frame_number,\n",
    "                                                                               atari.state, \n",
    "                                                                               MAIN_DQN,\n",
    "                                                                               evaluation=True)\n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "\n",
    "                if gif: \n",
    "                    frames_for_gif.append(new_frame)\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                    gif = False # Save only the first game of the evaluation as a gif\n",
    "                     \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n",
    "            try:\n",
    "                generate_gif(frame_number, frames_for_gif, eval_rewards[0], PATH)\n",
    "            except IndexError:\n",
    "                print(\"No evaluation game finished\")\n",
    "            \n",
    "            #Save the network parameters\n",
    "            saver.save(sess, PATH+'/my_model', global_step=frame_number)\n",
    "            frames_for_gif = []\n",
    "            \n",
    "            # Show the evaluation score in tensorboard\n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            with open('rewardsEval.dat', 'a') as eval_reward_file:\n",
    "                print(frame_number, np.mean(eval_rewards), file=eval_reward_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://gist.github.com/iganichev/d2d8a0b1abc6b15d4a07de83171163d4\n",
    "# Load variables in if they are relevant\n",
    "# Currently transfers conv layers\n",
    "# vars_list is a set of tf.trainable_variables\n",
    "def optimistic_restore(session, save_file, vars_list):\n",
    "    reader = tf.train.NewCheckpointReader(save_file)\n",
    "    saved_shapes = reader.get_variable_to_shape_map()\n",
    "    var_names = sorted([(var.name, var.name.split(':')[0]) for\n",
    "                      var in vars_list\n",
    "                      if var.name.split(':')[0] in saved_shapes])\n",
    "    restore_vars = []\n",
    "    name2var = dict(zip(map(lambda x: x.name.split(':')[0],\n",
    "                          tf.global_variables()),\n",
    "                      tf.global_variables()))\n",
    "    with tf.variable_scope('', reuse=True):\n",
    "        for var_name, saved_var_name in var_names:\n",
    "            curr_var = name2var[saved_var_name]\n",
    "            var_shape = curr_var.get_shape().as_list()\n",
    "            if var_shape == saved_shapes[saved_var_name]:\n",
    "                restore_vars.append(curr_var)\n",
    "                \n",
    "    print(\"Restoring the following variables: {}\".format(restore_vars))\n",
    "    saver = tf.train.Saver(restore_vars)\n",
    "    saver.restore(session, save_file)\n",
    "\n",
    "# Train on GPU if a trained_path is provided\n",
    "def transfer_initialized_train_dqn(trained_path = None, save_file = None, model_name=\"my_model\"):\n",
    "    # Trained path: The path (if provided) to look for the saved file from. EG: \"trained/pong/\"\n",
    "    # Save_file: The path (if provided) to save tf outputs to\n",
    "    # The model name\n",
    "    \"\"\"Contains the training and evaluation loops\"\"\"\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()  \n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BS)   # (★)\n",
    "    network_updater = TargetNetworkUpdater(MAIN_DQN_VARS, TARGET_DQN_VARS)\n",
    "    action_getter = ActionGetter(atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        if trained_path != None:\n",
    "            # Init everything to start off unitialized weights\n",
    "            sess.run(init)\n",
    "            # Load saver for weights that can be transferred\n",
    "            # saver = tf.train.import_meta_graph(trained_path+save_file)\n",
    "            optimistic_restore(sess, tf.train.latest_checkpoint(trained_path), [v for v in MAIN_DQN_VARS if \"conv\" in v.name]+[v for v in TARGET_DQN_VARS if \"conv\" in v.name])\n",
    "        \n",
    "        else:\n",
    "            sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            ########################\n",
    "            ####### Training #######\n",
    "            ########################\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    # (4★)\n",
    "                    action = action_getter.get_action(sess, frame_number, atari.state, MAIN_DQN)   \n",
    "                    # (5★)\n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)  \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    # (7★) Store transition in the replay memory\n",
    "                    my_replay_memory.add_experience(action=action, \n",
    "                                                    frame=processed_new_frame[:, :, 0],\n",
    "                                                    reward=reward, \n",
    "                                                    terminal=terminal_life_lost)   \n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DQN, TARGET_DQN, BS, gamma = DISCOUNT_FACTOR) # (8★)\n",
    "                        loss_list.append(loss)\n",
    "                    if frame_number % NETW_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        network_updater.update_networks(sess) # (9★)\n",
    "                    \n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                # Output the progress:\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Scalar summaries for tensorboard\n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                                   REWARD_PH:np.mean(rewards[-100:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                    # Histogramm summaries for tensorboard\n",
    "                    summ_param = sess.run(PARAM_SUMMARIES)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "                    \n",
    "                    print(len(rewards), frame_number, np.mean(rewards[-100:]))\n",
    "                    with open('rewards.dat', 'a') as reward_file:\n",
    "                        print(len(rewards), frame_number, np.mean(rewards[-100:]), file=reward_file)\n",
    "            \n",
    "            ########################\n",
    "            ###### Evaluation ######\n",
    "            ########################\n",
    "            terminal = True\n",
    "            gif = True\n",
    "            frames_for_gif = []\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "               \n",
    "                # Fire (action 1), when a life was lost or the game just started, \n",
    "                # so that the agent does not stand around doing nothing. When playing \n",
    "                # with other environments, you might want to change this...\n",
    "                action = 1 if terminal_life_lost else action_getter.get_action(sess, frame_number,\n",
    "                                                                               atari.state, \n",
    "                                                                               MAIN_DQN,\n",
    "                                                                               evaluation=True)\n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "\n",
    "                if gif: \n",
    "                    frames_for_gif.append(new_frame)\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                    gif = False # Save only the first game of the evaluation as a gif\n",
    "                     \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n",
    "            try:\n",
    "                generate_gif(frame_number, frames_for_gif, eval_rewards[0], PATH)\n",
    "            except IndexError:\n",
    "                print(\"No evaluation game finished\")\n",
    "            \n",
    "            #Save the network parameters\n",
    "            saver.save(sess, PATH+'/'+model_name, global_step=frame_number)\n",
    "            frames_for_gif = []\n",
    "            \n",
    "            # Show the evaluation score in tensorboard\n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            with open('rewardsEval.dat', 'a') as eval_reward_file:\n",
    "                print(frame_number, np.mean(eval_rewards), file=eval_reward_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    \n",
    "    gif_path = \"GIF/\"\n",
    "    os.makedirs(gif_path,exist_ok=True)\n",
    "\n",
    "    if ENV_NAME == 'BreakoutDeterministic-v4':\n",
    "        trained_path = \"trained/breakout/\"\n",
    "        save_file = \"my_model-15845555.meta\"\n",
    "    \n",
    "    elif ENV_NAME == 'PongDeterministic-v4':\n",
    "        trained_path = \"trained/pong/\"\n",
    "        save_file = \"my_model-3217770.meta\"\n",
    "\n",
    "    action_getter = ActionGetter(atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(trained_path+save_file)\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(trained_path))\n",
    "        frames_for_gif = []\n",
    "        terminal_live_lost = atari.reset(sess, evaluation = True)\n",
    "        episode_reward_sum = 0\n",
    "        while True:\n",
    "            atari.env.render()\n",
    "            action = 1 if terminal_live_lost else action_getter.get_action(sess, 0, atari.state, \n",
    "                                                                           MAIN_DQN, \n",
    "                                                                           evaluation = True)\n",
    "            processed_new_frame, reward, terminal, terminal_live_lost, new_frame = atari.step(sess, action)\n",
    "            episode_reward_sum += reward\n",
    "            frames_for_gif.append(new_frame)\n",
    "            if terminal == True:\n",
    "                break\n",
    "        \n",
    "        atari.env.close()\n",
    "        print(\"The total reward is {}\".format(episode_reward_sum))\n",
    "        print(\"Creating gif...\")\n",
    "        generate_gif(0, frames_for_gif, episode_reward_sum, gif_path)\n",
    "        print(\"Gif created, check the folder {}\".format(gif_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring the following variables: [<tf.Variable 'mainDQN/conv1/kernel:0' shape=(8, 8, 4, 32) dtype=float32_ref>, <tf.Variable 'mainDQN/conv2/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>, <tf.Variable 'mainDQN/conv3/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'mainDQN/conv4/kernel:0' shape=(7, 7, 64, 1024) dtype=float32_ref>, <tf.Variable 'targetDQN/conv1/kernel:0' shape=(8, 8, 4, 32) dtype=float32_ref>, <tf.Variable 'targetDQN/conv2/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>, <tf.Variable 'targetDQN/conv3/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'targetDQN/conv4/kernel:0' shape=(7, 7, 64, 1024) dtype=float32_ref>]\n",
      "10 1606 0.7\n",
      "20 3259 0.7\n",
      "30 5053 0.8666666666666667\n",
      "40 6806 0.925\n",
      "50 8542 0.92\n",
      "60 10151 0.8833333333333333\n",
      "70 11806 0.9\n",
      "80 13595 0.925\n",
      "90 15308 0.9333333333333333\n",
      "100 17111 0.95\n",
      "110 19052 1.04\n",
      "120 20685 1.04\n",
      "130 22697 1.08\n",
      "140 24480 1.09\n",
      "150 26337 1.14\n",
      "160 28172 1.21\n",
      "170 30202 1.27\n",
      "180 31973 1.25\n",
      "190 33539 1.21\n",
      "200 35108 1.17\n",
      "210 36993 1.15\n",
      "220 38524 1.13\n",
      "230 40261 1.06\n",
      "240 41940 1.03\n",
      "250 43652 0.98\n",
      "260 45479 0.97\n",
      "270 47135 0.88\n",
      "280 48822 0.87\n",
      "290 50507 0.9\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_TRANSFER:\n",
    "    if ENV_NAME == 'PongDeterministic-v4':\n",
    "        # Swap paths\n",
    "        trained_path = \"trained/breakout/\"\n",
    "        save_file = \"my_model-15845555.meta\"\n",
    "    \n",
    "    elif ENV_NAME == 'BreakoutDeterministic-v4':\n",
    "        # Swap paths\n",
    "        trained_path = \"trained/pong/\"\n",
    "        save_file = \"my_model-3217770.meta\"\n",
    "    # Only reset conv layers\n",
    "    transfer_initialized_train_dqn(trained_path = trained_path, \n",
    "                               save_file = save_file, \n",
    "                               model_name=\"transfer_learn_{}\".format(ENV_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
